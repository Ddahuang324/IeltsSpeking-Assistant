/**
 * Copyright 2024 Google LLC
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import { useCallback, useEffect, useMemo, useRef, useState } from "react";
import {
  MultimodalLiveAPIClientConnection,
  MultimodalLiveClient,
} from "../lib/multimodal-live-client";
import { LiveConfig, LiveOutgoingMessage, ServerContentMessage, RealtimeInputMessage, ClientContentMessage, ModelTurn, ServerContent } from "../multimodal-live-types";
import { AudioStreamer } from "../lib/audio-streamer";
import { audioContext } from "../lib/utils";
import VolMeterWorket from "../lib/worklets/vol-meter";
import { GenerativeContentBlob, Part } from "@google/generative-ai";
import { nanoid } from 'nanoid'
import { useVoskRecognition } from './use-vosk-recognition';


export type UseLiveAPIResults = {
  client: MultimodalLiveClient;
  setConfig: (config: LiveConfig) => void;
  config: LiveConfig;
  connected: boolean;
  connect: () => Promise<void>;
  disconnect: () => Promise<void>;
  volume: number;
  currentUserMessage: RealtimeInputMessage | ClientContentMessage | null;
  currentBotMessage: ServerContentMessage | null;
  currentTranscriptMessage: ServerContentMessage | null;
  setOutputMode: (mode: string) => void;
  transcribedText: string;
  setSpeechToTextEnabled: (enabled: boolean) => void;
};

export function useLiveAPI({
  url,
  apiKey,
}: MultimodalLiveAPIClientConnection): UseLiveAPIResults {
  const client = useMemo(
    () => new MultimodalLiveClient({ url, apiKey }),
    [url, apiKey],
  );
  const audioStreamerRef = useRef<AudioStreamer | null>(null);

  const [connected, setConnected] = useState(false);
  const [config, setConfig] = useState<LiveConfig>({
    model: "models/gemini-live-2.5-flash-preview",
  });
  const [volume, setVolume] = useState(0);
  const [outputMode, setOutputMode] = useState<string>('audio');
  const [speechToTextEnabled, setSpeechToTextEnabled] = useState<boolean>(true);
  // current message
  const [currentUserMessage, setCurrentUserMessage] = useState<RealtimeInputMessage | ClientContentMessage | null>(null);
  const [currentBotMessage, setCurrentBotMessage] = useState<ServerContentMessage | null>(null);
  const [currentTranscriptMessage, setCurrentTranscriptMessage] = useState<ServerContentMessage | null>(null);
  // è½¬å†™æ–‡æœ¬çŠ¶æ€
  const [transcribedText, setTranscribedText] = useState<string>('');
  // Voskè¯­éŸ³è¯†åˆ«
  console.log('ğŸ”§ useLiveAPI - Voské…ç½®:', { 
    outputMode, 
    speechToTextEnabled, 
    enabled: outputMode === 'audio_text' && speechToTextEnabled 
  });
  
  const { processAudioData, flush, isReady, error } = useVoskRecognition({
    enabled: outputMode === 'audio_text' && speechToTextEnabled,
    onResult: (text: string) => {
      console.log('ğŸ¯ Vosk æœ€ç»ˆç»“æœ:', text);
      setTranscribedText(text); // ç›´æ¥è®¾ç½®ï¼Œä¸ç´¯ç§¯
      // å°†æœ€ç»ˆè½¬å†™ç»“æœä½œä¸ºä¸€æ¡ ServerContentMessage æ¨å…¥ï¼Œä¾¿äºåœ¨èŠå¤©å†å²ä¸­è®°å½•
      setCurrentTranscriptMessage({
        serverContent: {
          modelTurn: {
            parts: [{ text }],
          },
        },
        id: nanoid(),
      });
    },
    onPartialResult: (text: string) => {
      console.log('ğŸ¤ Vosk éƒ¨åˆ†ç»“æœ:', text);
      setTranscribedText(text); // æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
    },
    onError: (error: string) => {
      console.error('âŒ Vosk é”™è¯¯:', error);
    }
  });
  
  console.log('ğŸ”§ useLiveAPI - VoskçŠ¶æ€:', { isReady, error, hasProcessAudioData: !!processAudioData });
  // æœåŠ¡ç«¯è¿”å›çš„è¯­éŸ³ï¼Œä¸€æ–¹é¢ç›´æ¥æ’­æ”¾ï¼Œå¦ä¸€æ–¹é¢éœ€è¦ä¿å­˜èµ·æ¥ï¼Œç»“æŸçš„æ—¶å€™ï¼Œç”Ÿæˆä¸€ä¸ªæ’­æ”¾åœ°å€
  const botAudioParts = useRef<Part[]>([]);
  const botContentParts = useRef<Part[]>([]);
  // ç”¨æˆ·è¾“å…¥çš„è¯­éŸ³/å›¾ç‰‡éœ€è¦ä¿å­˜èµ·æ¥ï¼Œç»“æŸçš„æ—¶å€™ç”Ÿæˆè¯­éŸ³/è§†é¢‘ï¼Ÿ
  const mediaChunks = useRef<GenerativeContentBlob[]>([]);

  // register audio for streaming server -> speakers
  useEffect(() => {
    if (!audioStreamerRef.current) {
      audioContext({ id: "audio-out" }).then((audioCtx: AudioContext) => {
        audioStreamerRef.current = new AudioStreamer(audioCtx);
        audioStreamerRef.current
          .addWorklet<any>("vumeter-out", VolMeterWorket, (ev: any) => {
            setVolume(ev.data.volume);
          })
          .then(() => {
            // Successfully added worklet
          });
      });
    }
  }, [audioStreamerRef]);

  useEffect(() => {
    const onClose = () => {
      setConnected(false);
    };

    const stopAudioStreamer = () => audioStreamerRef.current?.stop();

    const onAudio = (data: ArrayBuffer) => {
      if (outputMode !== 'text') {
        if (outputMode === 'audio') {
          audioStreamerRef.current?.addPCM16(new Uint8Array(data));
        } else if (outputMode === 'audio_text') {
          // Audio+Textæ¨¡å¼ï¼šæ’­æ”¾éŸ³é¢‘å¹¶è¿›è¡Œè¯­éŸ³è½¬æ–‡å­—
          console.log('ğŸ”Š Audio+Textæ¨¡å¼ - æ’­æ”¾éŸ³é¢‘:', { dataLength: data.byteLength });
          audioStreamerRef.current?.addPCM16(new Uint8Array(data));
          // å°†PCM16éŸ³é¢‘æ•°æ®å‘é€ç»™Voskè¿›è¡Œè½¬å†™ï¼ˆä»…åœ¨å¯ç”¨æ—¶ï¼‰
          console.log('ğŸ¤ æ£€æŸ¥VoskçŠ¶æ€:', { isReady, speechToTextEnabled, outputMode });
          if (isReady && speechToTextEnabled) {
            console.log('ğŸ“¤ å‘é€éŸ³é¢‘æ•°æ®ç»™Voskï¼Œä½¿ç”¨24000Hzé‡‡æ ·ç‡');
            // Geminiè¿”å›çš„éŸ³é¢‘é‡‡æ ·ç‡æ˜¯24000Hzï¼Œéœ€è¦ä¼ é€’ç»™Vosk
            processAudioData(data, 24000);
          } else {
            console.log('âš ï¸ Voskæœªå‡†å¤‡å¥½æˆ–å·²ç¦ç”¨');
          }
        }
      }
    }

    client
      .on("close", onClose)
      .on("interrupted", stopAudioStreamer)
      .on("audio", onAudio);

    return () => {
      client
        .off("close", onClose)
        .off("interrupted", stopAudioStreamer)
        .off("audio", onAudio);
    };
  }, [client, outputMode, isReady, processAudioData, speechToTextEnabled]);

  useEffect(() => {
    let currnetBotMessageId: string = nanoid()
    let currnetUserMessageId: string = nanoid()
    // const onAudio = (data: ArrayBuffer) => {
    //   // ä¿å­˜ç»“æœåˆ°botAudioBuffers
    //   botAudioBuffers.current?.push(data)
    // }
    const onAudioContent = (data: ModelTurn['modelTurn']['parts']) => {
      // ä¿å­˜ç»“æœåˆ°botAudioParts
      botAudioParts.current = [...botAudioParts.current, ...data]
    }
    const onInput = (data: RealtimeInputMessage | ClientContentMessage) => {
      if ((data as RealtimeInputMessage)?.realtimeInput?.mediaChunks) {
        mediaChunks.current?.push(...(data as RealtimeInputMessage)?.realtimeInput?.mediaChunks)
      }
      if ((data as ClientContentMessage)?.clientContent) {
        // ç”¨æˆ·è¾“å…¥äº†å°±ä¼šæœ‰ä¸€ä¸ªturnCompleteï¼Œç«‹å³ç»“æŸ
        setCurrentUserMessage({
          ...data,
          id: currnetUserMessageId,
          // å…ˆä¸å¤„ç†è¾“å…¥çš„è¯­éŸ³æ¶ˆæ¯
          // realtimeInput: {
          //   mediaChunks: mediaChunks?.current ?? [],
          // }
        })
        currnetUserMessageId = nanoid()  // ç”Ÿæˆä¸€ä¸ªæ–°çš„id
        mediaChunks.current = []  // æ¸…ç©ºmediaChunks
      }
    }
    const onContent = (content: ModelTurn) => {
      // æ–‡æœ¬è¾“å‡ºï¼Œå°†æ–‡æœ¬æ”¾åˆ°bot messageé‡Œé¢
      if (content.modelTurn?.parts) {
        botContentParts.current.push(...content.modelTurn?.parts)  
        // è¿™é‡Œéœ€è¦å…ˆè®¾ç½®æ–‡æœ¬æ¶ˆæ¯ï¼Œæ”¯æŒå®æ—¶çš„æ‰“å­—æœºæ•ˆæœ
        setCurrentBotMessage({
          serverContent: {
            modelTurn: {
              // è¿™é‡Œåªæœ‰æ–‡æœ¬æ¶ˆæ¯ï¼Œè¯­éŸ³æ¶ˆæ¯åªåœ¨æœ€åæ”¶åˆ°turncompleteçš„æ—¶å€™å†ä¸€æ¬¡æ€§å‘è¿‡å»
              parts: botContentParts.current,
            }
          },
          id: currnetBotMessageId,
        })
      }
		}
		const onInterrupted = () => {
			// è¿™ä¸ªäº‹ä»¶åº”è¯¥è¡¨ç¤ºçš„æ˜¯ï¼Œæœºå™¨äººçš„è¯­éŸ³æ¶ˆæ¯è¢«æ‰“æ–­ï¼Ÿå®é™…ä¸Šåº”è¯¥ç®—ç”¨æˆ·è¯­éŸ³è¾“å…¥å¼€å§‹
			console.log('onInterrupted')
			// if (buffers.length) {
			// 	new Blob(buffers).arrayBuffer().then((buffer: ArrayBuffer) => {
			// 		const blob = pcmBufferToBlob(buffer);
			// 		const audioUrl = URL.createObjectURL(blob);
			// 		const message = { audioUrl }
			// 		setMessages((state: any) => {
			// 			console.log('new message', state, message)
			// 			return [...state, message]
			// 		})
			// 	})
			// }
		}
		const onTurnComplete = () => {
			// è¿™ä¸ªäº‹ä»¶è¡¨ç¤ºæœºå™¨äººç”Ÿæˆçš„æ¶ˆæ¯ç»“æŸäº†ï¼Œä¸ç®¡æ˜¯æ–‡æœ¬ç»“æŸè¿˜æ˜¯è¯­éŸ³ç»“æŸï¼Œéƒ½æœ‰è¿™ä¸ªæ¶ˆæ¯
			console.log('onTurnComplete')
			if (botContentParts.current?.length || botAudioParts.current?.length) {
        setCurrentBotMessage({
          serverContent: {
            modelTurn: {
              // æ–‡æœ¬æ¶ˆæ¯åŠ ä¸Šè¯­éŸ³æ¶ˆæ¯
              parts: [...botContentParts.current, ...botAudioParts.current],
            }
          },
          id: currnetBotMessageId,
        })
        currnetBotMessageId = nanoid()
        botContentParts.current = []; // æ¸…ç©ºæ•°æ®
        botAudioParts.current = [];
			}
      // å½“æœ¬è½®å¯¹è¯çš„éŸ³é¢‘è¾“å‡ºç»“æŸæ—¶ï¼Œè§¦å‘ä¸€æ¬¡Vosk flushï¼Œè·å–æœ€ç»ˆè½¬å†™ç»“æœ
      if (speechToTextEnabled && isReady) {
        try {
          flush();
        } catch (e) {
          console.warn('flush è°ƒç”¨å¤±è´¥', e);
        }
      }
		}
    client
      .on('interrupted', onInterrupted)
      .on('turncomplete', onTurnComplete)
      .on('content', onContent)
      .on('input', onInput)
      .on('audiocontent', onAudioContent);
    return () => {
      client
        .off('interrupted', onInterrupted)
        .off('turncomplete', onTurnComplete)
        .off('content', onContent)
        .off('input', onInput)
        .off('audiocontent', onAudioContent);
    }
  }, [client, flush, isReady, speechToTextEnabled])

  const connect = useCallback(async () => {
    console.log(config);
    if (!config) {
      throw new Error("config has not been set");
    }
    client.disconnect();
    try {
      await client.connect(config);
      setConnected(true);
      // æ¸…ç©ºä¹‹å‰çš„è½¬å½•æ–‡æœ¬ä¸ä¸´æ—¶è½¬å†™æ¶ˆæ¯
      setTranscribedText('');
      setCurrentTranscriptMessage(null);
    } catch (err: any) {
      // å°†é”™è¯¯æŠ›å‡ºç»™è°ƒç”¨æ–¹ï¼ˆé¡µé¢ï¼‰ä»¥ä¾¿å¼¹çª—æç¤º
      console.error('connect failed:', err);
      setConnected(false);
      throw err;
    }
  }, [client, setConnected, config]);

  const disconnect = useCallback(async () => {
    client.disconnect();
    setConnected(false);
  }, [setConnected, client]);

  const setOutputModeCallback = useCallback((mode: string) => {
    setOutputMode(mode);
  }, []);

  return {
    client,
    config,
    setConfig,
    connected,
    connect,
    disconnect,
    volume,
    currentUserMessage,
    currentBotMessage,
    currentTranscriptMessage,
    setOutputMode: setOutputModeCallback,
    transcribedText,
    setSpeechToTextEnabled,
  };
}
